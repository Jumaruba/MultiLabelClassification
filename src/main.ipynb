{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3c8b067af59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "#train models \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up stats dataframe\n",
    "import time\n",
    "stats = pd.Dataframe(columns=['Algorithm','Label','Accurary','Precision','Recall','F1','Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equals to empty\", len(train.loc[train['ABSTRACT'] == \"\"]))\n",
    "print(\"Equals to none\", len(train.loc[train['ABSTRACT'] == None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most common category \n",
    "cols = train.columns.tolist() \n",
    "cols = cols[3:]\n",
    "\n",
    "train_sum = train[cols].sum() \n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(cols, train_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(train[cols].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of abstract text + title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there's a text really large, we might cut it's size \n",
    "len_abstract = [len(i.split(' ')) for i in train['ABSTRACT']]\n",
    "len_title = [len(i.split(' ')) for i in train['TITLE']]\n",
    "len_text = [len_title[i] + len_abstract[i] for i in range(len(train))]\n",
    "x = [i for i in range(len(train))]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.scatter(x, len_text, s=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "mostUsed = pd.Series(' '.join(train['ABSTRACT']).lower().split()).value_counts()[:10]\n",
    "mostUsedGraph = sn.barplot(mostUsed.index, mostUsed.values)\n",
    "mostUsedGraph.set(xlabel=\"Words\", ylabel=\"Occurrencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(df): \n",
    "    corpus = []\n",
    "    ps = PorterStemmer()\n",
    "    df_size = len(df) \n",
    "    for i in range(0,df_size):\n",
    "        # get review and remove non alpha chars\n",
    "        article = re.sub('[^a-zA-Z]', ' ', df['ABSTRACT'][i])\n",
    "        # to lower-case and tokenize\n",
    "        article = article.lower().split()\n",
    "        # stemming and stop word removal\n",
    "        article = ' '.join([ps.stem(w) for w in article if not w in set(stopwords.words('english'))])\n",
    "        corpus.append(article)\n",
    "    return corpus\n",
    "\n",
    "# To avoid making this cleaning every time, we save teh output as a csv\n",
    "def save_csv(corpus_train, file_name): \n",
    "    df = pd.DataFrame({'text': corpus_train})\n",
    "    df.to_csv(file_name) \n",
    "    \n",
    "train['ABSTRACT'] = train['TITLE'] +train['ABSTRACT']\n",
    "corpus_train = normalize(train)\n",
    "save_csv(corpus_train, 'preprocessed_train.csv')\n",
    "\n",
    "test['ABSTRACT'] = test['TITLE'] + test['ABSTRACT']\n",
    "corpus_test = normalize(test)\n",
    "save_csv(corpus_test, 'preprocessed_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(preprocessed, x_col_name): \n",
    "    vectorizer = CountVectorizer(max_features = 1500)\n",
    "\n",
    "    X_array = preprocessed.loc[:,x_col_name]\n",
    "    X = vectorizer.fit_transform(X_array).toarray()\n",
    "    return X\n",
    "\n",
    "def get_outputs(df, y_cols_name):\n",
    "    y = []\n",
    "    for col_name in y_cols_name: \n",
    "        col_values = df.loc[:,col_name].values\n",
    "        y.append(col_values)\n",
    "    return y\n",
    "\n",
    "\n",
    "preprocessed = pd.read_csv('preprocessed_train.csv')\n",
    "preprocessed_submission = pd.read_csv('preprocessed_test.csv')\n",
    "y_columns = train.columns[3:]\n",
    "X = get_input(preprocessed, 'text')\n",
    "y = get_outputs(train, y_columns)\n",
    "X_submission = get_input(preprocessed_submission, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_test = []\n",
    "for i in range(len(y_columns)): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y[i], test_size = 0.20, random_state = 0)\n",
    "    y_train_test.append([y_train, y_test])\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(y_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling \n",
    "Run this cell only with it's desired to run the oversamplying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12)\n",
    "quant_biology_train_y = y_train_test[4][0];\n",
    "quant_finance_train_y = y_train_test[5][0];  \n",
    "\n",
    "x_biology, y_train_test[4][0] = sm.fit_resample(X_train, quant_biology_train_y)\n",
    "x_finance, y_train_test[5][0] = sm.fit_resample(X_train, quant_finance_train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(y_test, y_pred, column_name): \n",
    "   # print(confusion_matrix(y_test, y_pred))\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1: ', f1_score(y_test, y_pred))\n",
    "    return [accuracy_score(y_test, y_pred),precision_score(y_test, y_pred),recall_score(y_test, y_pred),f1_score(y_test, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_train(index, with_oversampling):\n",
    "    if index == 4 and with_oversampling: \n",
    "        return x_biology\n",
    "    elif index == 5 and with_oversampling: \n",
    "        return x_finance\n",
    "    return X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "\n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    x_train = get_x_train(i) \n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test, with_oversampling)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array = print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Naive Bayes',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "\n",
    "index_submission= test.loc[:, 'ID'].values\n",
    "print(len(gaussian_submission_df))\n",
    "gaussian_submission_df = gaussian_submission_df.set_index(pd.Index(index_submission), 'ID')\n",
    "gaussian_df.index.name = \"ID\"    \n",
    "print(gaussian_submission_df)\n",
    "gaussian_df.to_csv(\"submission_gaussian.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "classifier = LinearSVC()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array = print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Support Vector Machines',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "    \n",
    "    \n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array = print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Logistic Regression',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "    \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array = print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Perceptron',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "    \n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Decision Tree',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "    \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "gaussian_df = pd.DataFrame()\n",
    "gaussian_submission_df = pd.DataFrame() \n",
    "for i in range(len(y_columns)): \n",
    "    col_name = y_columns[i]\n",
    "    print(\"COL:\", col_name)\n",
    "    y_train = y_train_test[i][0]\n",
    "    y_test = y_train_test[i][1]\n",
    "    \n",
    "    begin = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    gaussian_df[col_name]= classifier.predict(X_test)\n",
    "    gaussian_submission_df[col_name] = classifier.predict(X_submission)\n",
    "    stat_array = print_statistics(y_test, gaussian_df[col_name].values, col_name)\n",
    "    print()\n",
    "    stats.append(['Random Forest',col_name,stat_array[0],stat_array[1],stat_array[2],stat_array[3],end-begin])\n",
    "    \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export stats to csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
